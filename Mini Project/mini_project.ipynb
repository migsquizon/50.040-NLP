{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mini_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cernoBq6CdX9",
        "bDOUuC7-CdX-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pf_8OI34CdXU"
      },
      "source": [
        "<img src=\"sutd.png\" alt=\"drawing\" style=\"width:300px;\"/>\n",
        "\n",
        "## <center>50.040 Natural Language Processing, Summer 2020<center>\n",
        "<center>**Due 19 June 2020, 5pm** <center>\n",
        "Mini Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YAN5MA5GCdXU"
      },
      "source": [
        "**Write your student ID and name**\n",
        "\n",
        "\n",
        "### STUDENT ID: 1003014\n",
        "\n",
        "### Name: Antonio Miguel Canlas Quizon\n",
        "\n",
        "### Students with whom you have discussed (if any):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0iF5uWcCdXV"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Language models are very useful for a wide range of applications, e.g., speech recognition and machine translation. Consider a sentence consisting of words $x_1, x_2, …, x_m$, where $m$ is the length of the sentence, the goal of language modeling is to model the probability of the sentence, where $m \\geq 1$, $x_i \\in V $ and $V$ is the vocabulary of the corpus:\n",
        "$$p(x_1, x_2, …, x_m)$$\n",
        "In this project, we are going to explore both statistical language model and neural language model on the [Wikitext-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) datasets. Download wikitext-2 word-level data and put it under the ``data`` folder.\n",
        "\n",
        "## Statistical  Language Model\n",
        "\n",
        "A simple way is to view words as independent random variables (i.e., zero-th order Markovian assumption). The joint probability can be written as:\n",
        "$$p(x_1, x_2, …, x_m)=\\prod_{i=1}^m p(x_i)$$\n",
        "However, this model ignores the word order information, to account for which, under the first-order Markovian assumption, the joint probability can be written as:\n",
        "$$p(x_0, x_1, x_2, …, x_m)= \\prod_{i=1}^{m}p(x_i \\mid x_{i-1})$$\n",
        "Under the second-order Markovian assumption, the joint probability can be written as:\n",
        "$$p(x_{-1}, x_0, x_1, x_2, …, x_m)= \\prod_{i=1}^{m}p(x_i \\mid x_{i-2}, x_{i-1})$$\n",
        "Similar to what we did in HMM, we will assume that $x_{-1}=START, x_0=START, x_m = STOP$ in this definition, where $START, STOP$ are special symbols referring to the start and the end of a sentence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F5wC4lqKCdXW"
      },
      "source": [
        "### Parameter estimation\n",
        "\n",
        "Let's use $count(u)$ to denote the number of times the unigram $u$ appears in the corpus, use $count(v, u)$ to denote the number of times the bigram $v, u$ appears in the corpus, and $count(w, v, u)$ the times the trigram $w, v, u$ appears in the corpus, $u \\in V \\cup STOP$ and $w, v \\in V \\cup START$.\n",
        "\n",
        "And the parameters of the unigram, bigram and trigram models can be obtained using maximum likelihood estimation (MLE).\n",
        "\n",
        "- In the unigram model, the parameters can be estimated as: $$p(u) = \\frac {count(u)}{c}$$, where $c$ is the total number of words in the corpus.\n",
        "- In the bigram model, the parameters can be estimated as:\n",
        "$$p(u \\mid v) = \\frac{count(v, u)}{count(v)}$$\n",
        "- In the trigram model, the parameters can be estimated as:\n",
        "$$p(u \\mid w, v) = \\frac{count(w, v, u)}{count(w, v)}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5jc8zzLF-1X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1b30d2dd-827b-45ab-d78e-39542854bf4f"
      },
      "source": [
        "%%javascript\n",
        "MathJax.Hub.Config({\n",
        "  TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
        "});"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "MathJax.Hub.Config({\n",
              "  TeX: { equationNumbers: { autoNumber: \"AMS\" } }\n",
              "});"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C7dks8t0CdXW"
      },
      "source": [
        "### Smoothing the parameters\n",
        "Note, it is likely that many parameters of bigram and trigram models will be 0 because the relevant bigrams and trigrams involved do not appear in the corpus. If you don't have a way to handle these 0 probabilities, all the sentences that include such bigrams or trigrams will have probabilities of 0.\n",
        "\n",
        "We'll use a Add-k Smoothing method to fix this problem, the smoothed parameter can be estimated as:\n",
        "\\begin{equation}\n",
        "p_{add-k}(u)= \\frac{count(u)+k}{c+k|V^*|}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "p_{add-k}(u \\mid v)= \\frac{count(v, u)+k}{count(v)+k|V^*|}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "p_{add-k}(u \\mid w, v)= \\frac{count(w, v, u)+k}{count(w, v)+k|V^*|}\n",
        "\\end{equation}\n",
        "\n",
        "where $k \\in (0, 1)$ is the parameter of this approach, and $|V^*|$ is the size of the vocabulary $V^*$,here $V^*= V \\cup STOP$. One way to choose the value of $k$ is by\n",
        "optimizing the perplexity of the development set, namely to choose the value that minimizes the perplexity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SNVha_8UCdXX"
      },
      "source": [
        "### Perplexity\n",
        "\n",
        "Given a test set $D^{\\prime}$ consisting of sentences $X^{(1)}, X^{(2)}, …, X^{(|D^{\\prime}|)}$, each sentence $X^{(j)}$ consists of words $x_1^{(j)}, x_2^{(j)},…,x_{n_j}^{(j)}$, we can measure the probability of each sentence $s_i$, and the quality of the language model would be the probability it assigns to the entire set of test sentences, namely:\n",
        "\\begin{equation} \n",
        "\\prod_j^{D^{\\prime}}p(X^{(j)})\n",
        "\\end{equation}\n",
        "Let's define average log2 probability as:\n",
        "\\begin{equation} \n",
        "l=\\frac{1}{c^{\\prime}}\\sum_{j=1}^{|D^{\\prime}|}log_2p(X^{(j)})\n",
        "\\end{equation}\n",
        "$c^{\\prime}$ is the total number of words in the test set, $D^{\\prime}$ is the number of sentences. And the perplexity is defined as:\n",
        "\\begin{equation} \n",
        "perplexity=2^{-l}\n",
        "\\end{equation}\n",
        "\n",
        "The lower the perplexity, the better the language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taTRFLiUf-xu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ee7e41a-a360-4d2d-93d6-ed7e221b4e76"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16J7JRzbgLqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2082c776-652e-4653-9fa9-8e84885887ff"
      },
      "source": [
        "% pwd\n",
        "% ls\n",
        "% cd /content/gdrive/'My Drive'/'NLP'/'Mini Project'\n",
        "% ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n",
            "/content/gdrive/My Drive/NLP/Mini Project\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/            greedy.png  \u001b[01;34m__MACOSX\u001b[0m/           sutd.png\n",
            "Description.pdf  LM.png      mini_project.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCMpP0Y-F-1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, namedtuple\n",
        "import itertools\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuA5HcnPF-1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data/wikitext-2-v1/wikitext-2/wiki.train.tokens', 'r', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "    train_sents = [line.lower().strip('\\n').split() for line in text]\n",
        "    train_sents = [s for s in train_sents if len(s)>0 and s[0] != '=']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSlu9u4CF-1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "753cceaf-0677-4196-cd69-57acb09c56aa"
      },
      "source": [
        "print(train_sents[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'valkyria', 'chronicles', 'ii', '.', 'while', 'it', 'retained', 'the', 'standard', 'features', 'of', 'the', 'series', ',', 'it', 'also', 'underwent', 'multiple', 'adjustments', ',', 'such', 'as', 'making', 'the', 'game', 'more', '<unk>', 'for', 'series', 'newcomers', '.', 'character', 'designer', '<unk>', 'honjou', 'and', 'composer', 'hitoshi', 'sakimoto', 'both', 'returned', 'from', 'previous', 'entries', ',', 'along', 'with', 'valkyria', 'chronicles', 'ii', 'director', 'takeshi', 'ozawa', '.', 'a', 'large', 'team', 'of', 'writers', 'handled', 'the', 'script', '.', 'the', 'game', \"'s\", 'opening', 'theme', 'was', 'sung', 'by', 'may', \"'n\", '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G0vxL-WpCdXX"
      },
      "source": [
        "### Question 1 [code][written]\n",
        "1. Implement the function **\"compute_ngram\"** that computes n-grams in the corpus.\n",
        " (Do not take the START and STOP symbols into consideration for now.) \n",
        " For n=1,2,3, the number of unique n-grams should be **28910/577343/1344047**, respectively.\n",
        "2. List 10 most frequent unigrams, bigrams and trigrams as well as their counts.(Hint: use the built-in function .most_common in Counter class)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TOyoT-QiCdXY",
        "colab": {}
      },
      "source": [
        "# en_y_to_x = dict()\n",
        "# for i in numpy_en[:]:\n",
        "#     en_y_to_x[i[0]+i[1]] = en_y_to_x.get(i[0]+i[1], 0) + 1\n",
        "\n",
        "def compute_ngram(sents, n):\n",
        "    '''\n",
        "    Compute n-grams that appear in \"sents\".\n",
        "    param:\n",
        "        sents: list[list[str]] --- list of list of word strings\n",
        "        n: int --- \"n\" gram\n",
        "    return:\n",
        "        ngram_set: set{str} --- a set of n-grams (no duplicate elements)\n",
        "        ngram_dict: dict{ngram: counts} --- a dictionary that maps each ngram to its number occurence in \"sents\";\n",
        "        This dict contains the parameters of our ngram model. E.g. if n=2, ngram_dict={('a','b'):10, ('b','c'):13}\n",
        "        \n",
        "        You may need to use \"Counter\", \"tuple\" function here.\n",
        "    '''\n",
        "    ngram_set = None\n",
        "    ngram_dict = None\n",
        "    ### YOUR CODE HERE\n",
        "    ngram_dict = dict()\n",
        "    for sentence in range(len(sents)):\n",
        "      # print(sents[sentence])\n",
        "      # temp_tuple = tuple()\n",
        "      ngrams = list(zip(*[sents[sentence][i:] for i in range(n)]))\n",
        "      # print(ngrams)\n",
        "      for ng in range(len(ngrams)):\n",
        "        ngram_dict[ngrams[ng]] = ngram_dict.get(ngrams[ng], 0) + 1\n",
        "    \n",
        "\n",
        "    ngram_set = ngram_dict.keys()\n",
        "      # for i in range(len(sents[sentence]-n+1)):\n",
        "      #   # print(sents[sentence][i])\n",
        "      #   ngram_dict[sents[sentence][i]] = ngram_dict.get(sents[sentence][i], 0) + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    return ngram_set, ngram_dict"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFUajhyzF-1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a2d8145-42f5-441d-b8a2-c3412f8c1435"
      },
      "source": [
        "\n",
        "### ~28xxx\n",
        "unigram_set, unigram_dict = compute_ngram(train_sents, 1)\n",
        "print(len(unigram_set))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28910\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAgTpHh2F-1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1429be16-05c7-425d-ba87-fe82f9884094"
      },
      "source": [
        "### ~57xxxx\n",
        "bigram_set, bigram_dict = compute_ngram(train_sents, 2)\n",
        "print(len(bigram_set))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "577343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhW5ksHNF-1o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4bfa1f2-81bb-4df5-a113-d991bdf482fb"
      },
      "source": [
        "### ~134xxxx\n",
        "trigram_set, trigram_dict = compute_ngram(train_sents, 3)\n",
        "print(len(trigram_set))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1344047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbUHVZ1UF-1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "fb8101e7-249f-48ca-d68f-8e891270c35a"
      },
      "source": [
        "# List 10 most frequent unigrams, bigrams and trigrams as well as their counts.\n",
        "# print(unigram_dict)\n",
        "print('----------------- UNIGRAM---------------')\n",
        "unigram_counter = Counter(unigram_dict)\n",
        "print(unigram_counter.most_common(10))\n",
        "print('----------------- BIGRAM---------------')\n",
        "bigram_counter = Counter(bigram_dict)\n",
        "print(bigram_counter.most_common(10))\n",
        "print('----------------- TRIGRAM---------------')\n",
        "trigram_counter = Counter(trigram_dict)\n",
        "print(trigram_counter.most_common(10))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------- UNIGRAM---------------\n",
            "[(('the',), 130519), ((',',), 99763), (('.',), 73388), (('of',), 56743), (('<unk>',), 53951), (('and',), 49940), (('in',), 44876), (('to',), 39462), (('a',), 36140), (('\"',), 28285)]\n",
            "----------------- BIGRAM---------------\n",
            "[(('of', 'the'), 17242), (('in', 'the'), 11778), ((',', 'and'), 11643), (('.', 'the'), 11274), ((',', 'the'), 8024), (('<unk>', ','), 7698), (('to', 'the'), 6009), (('on', 'the'), 4495), (('the', '<unk>'), 4389), (('and', 'the'), 4331)]\n",
            "----------------- TRIGRAM---------------\n",
            "[((',', 'and', 'the'), 1393), ((',', '<unk>', ','), 950), (('<unk>', ',', '<unk>'), 901), (('one', 'of', 'the'), 866), (('<unk>', ',', 'and'), 819), (('.', 'however', ','), 775), (('<unk>', '<unk>', ','), 745), (('.', 'in', 'the'), 726), (('.', 'it', 'was'), 698), (('the', 'united', 'states'), 666)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR5uCs-Qz60M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(unigram_dict[('<START>',)])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PVpoDgU7CdXb"
      },
      "source": [
        "### Question 2 [code][written]\n",
        "In this part, we take the START and STOP symbols into consideration. So we need to pad the **train_sents** as described in \"Statistical Language Model\" before we apply \"compute_ngram\" function. For example, given a sentence \"I like NLP\", in a bigram model, we need to pad it as \"START I like NLP STOP\", in a trigram model, we need to pad it as \"START START I like NLP STOP\".\n",
        "\n",
        "1. Implement the ``pad_sents function``.\n",
        "2. Pad ``train_sents``.\n",
        "3. Apply ``compute_ngram`` function to these padded sents. \n",
        "4. Implement ``ngram_prob`` function. Compute the probability for each n-gram in the variable **ngrams** according to Eq.(1)(2)(3) in **\"smoothing the parameters\"** .List down the n-grams that have 0 probability. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MetlagQCF-1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88b07731-6fe2-452f-cd4c-ae13f7c8d23b"
      },
      "source": [
        "###############################################\n",
        "ngrams = list()\n",
        "with open(r'data/ngram.txt','r') as f:\n",
        "    for line in f:\n",
        "        ngrams.append(line.strip('\\n').split())\n",
        "print(ngrams)\n",
        "###############################################"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['the', 'computer'], ['go', 'to'], ['have', 'had'], ['and', 'the'], ['can', 'sea'], ['a', 'number', 'of'], ['with', 'respect', 'to'], ['in', 'terms', 'of'], ['not', 'good', 'bad'], ['first', 'start', 'with']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Vrv6UgDF-1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "START = '<START>'\n",
        "STOP = '<STOP>'\n",
        "###################################\n",
        "def pad_sents(sents, n):\n",
        "    '''\n",
        "    Pad the sents according to n.\n",
        "    params:\n",
        "        sents: list[list[str]] --- list of sentences.\n",
        "        n: int --- specify the padding type, 1-gram, 2-gram, or 3-gram.\n",
        "    return:\n",
        "        padded_sents: list[list[str]] --- list of padded sentences.\n",
        "    '''\n",
        "    padded_sents = None\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    padded_sents = copy.deepcopy(sents)\n",
        "    if n==1:\n",
        "      return sents\n",
        "    for sentence in range(len(sents)):\n",
        "      for i in range(n-1):\n",
        "        padded_sents[sentence].insert(i, START)\n",
        "      padded_sents[sentence].append(STOP)\n",
        "    # print(padded_sents[0])\n",
        "    \n",
        "    ### END OF YOUR CODE\n",
        "    return padded_sents"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7nDVmrzF-1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uni_sents = pad_sents(train_sents, 1)\n",
        "bi_sents = pad_sents(train_sents, 2)\n",
        "tri_sents = pad_sents(train_sents, 3)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA2G-NL0F-1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigram_set, unigram_dict = compute_ngram(uni_sents, 1)\n",
        "bigram_set, bigram_dict = compute_ngram(bi_sents, 2)\n",
        "trigram_set, trigram_dict = compute_ngram(tri_sents, 3)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gYefBNvF-11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e68f0cc-a8bd-4992-c894-667962367f2a"
      },
      "source": [
        "### (28xxx, 58xxxx, 136xxxx)\n",
        "len(unigram_set),len(bigram_set),len(trigram_set)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28910, 580825, 1363266)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4GeiN00F-14",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01b21710-0e46-408e-e0ae-cd90209939b7"
      },
      "source": [
        "### ~ 200xxxx; total number of words in wikitext-2.train\n",
        "num_words = sum([v for _,v in unigram_dict.items()])\n",
        "print(num_words)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2007146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85kjNCuTItxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_RhRbxyF-15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngram_prob(ngram, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
        "    '''\n",
        "    params:\n",
        "        ngram: list[str] --- a list that represents n-gram\n",
        "        num_words: int --- total number of words\n",
        "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
        "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
        "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
        "    return:\n",
        "        prob: float --- probability of the \"ngram\"\n",
        "    '''\n",
        "    prob = None\n",
        "    ### YOUR CODE HERE\\\n",
        "    if len(ngram) == 1:\n",
        "      if (ngram[0],) not in unigram_dict:\n",
        "        return 0.0\n",
        "      prob = unigram_dic[(ngram[0],)]/num_words\n",
        "    elif len(ngram) == 2:\n",
        "      if (ngram[0],) not in unigram_dict:\n",
        "        return 0.0\n",
        "      if (ngram[0],ngram[1]) not in bigram_dict:\n",
        "        return 0.0\n",
        "      prob = bigram_dict[(ngram[0],ngram[1])]/unigram_dic[(ngram[0],)]\n",
        "    elif len(ngram) == 3:\n",
        "      if (ngram[0],ngram[1]) not in bigram_dict:\n",
        "        return 0.0\n",
        "      if (ngram[0],ngram[1],ngram[2]) not in trigram_dict:\n",
        "        return 0.0\n",
        "      prob = trigram_dict[(ngram[0],ngram[1],ngram[2])]/bigram_dict[(ngram[0],ngram[1])]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    ### END OF YOUR CODE\n",
        "    return prob"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hiJsv70F-18",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "69c40255-c18e-4f94-8f56-4cd1412daf8b"
      },
      "source": [
        "### ~9.96e-05\n",
        "ngram_prob(ngrams[0], num_words,unigram_dict, bigram_dict, trigram_dict)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.960235674499498e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0afXmGn4F-1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a607cae6-3b56-4bdb-fa76-4180aa70c6c8"
      },
      "source": [
        "### List down the n-grams that have 0 probability.\n",
        "for ng in ngrams:\n",
        "  if ngram_prob(ng, num_words,unigram_dict, bigram_dict, trigram_dict) ==0:\n",
        "    print(ng)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['can', 'sea']\n",
            "['not', 'good', 'bad']\n",
            "['first', 'start', 'with']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2kAezpJ9CdXd"
      },
      "source": [
        "### Question 3 [code][written]\n",
        "\n",
        "1. Implement ``smooth_ngram_prob`` function to estimate ngram probability with ``add-k`` smoothing technique. Compute the smoothed probabilities of each n-gram in the variable **\"ngrams\"** according to Eq.(1)(2)(3) in **\"smoothing the parameters\"** section.\n",
        "2. Implement ``perplexity`` function to compute the perplexity of the corpus \"**valid_sents**\" according to the Equations (4),(5),(6) in **perplexity** section. The computation of $p(X^{(j)})$ depends on the n-gram model you choose. If you choose 2-gram model, then you need to calculate $p(X^{(j)})$ based on Eq.(2) in **smoothing the parameter** section. Hint: convert probability to log probability.\n",
        "3. Try out different $k\\in [0.1, 0.3, 0.5, 0.7, 0.9]$ and different n-gram model ($n=1,2,3$). Find the n-gram model and $k$ that gives the best perplexity on \"**valid_sents**\" (smaller is better)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfQb7aGNF-2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data/wikitext-2-v1/wikitext-2/wiki.valid.tokens', 'r', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "    valid_sents = [line.lower().strip('\\n').split() for line in text]\n",
        "    valid_sents = [s for s in valid_sents if len(s)>0 and s[0] != '=']\n",
        "\n",
        "uni_valid_sents = pad_sents(valid_sents, 1)\n",
        "bi_valid_sents = pad_sents(valid_sents, 2)\n",
        "tri_valid_sents = pad_sents(valid_sents, 3)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lgb5o3bF-2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth_ngram_prob(ngram, k, num_words, unigram_dic, bigram_dic, trigram_dic):\n",
        "    '''\n",
        "    params:\n",
        "        ngram: list[str] --- a list that represents n-gram\n",
        "        k: float \n",
        "        num_words: int --- total number of words\n",
        "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
        "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
        "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
        "    return:\n",
        "        s_prob: float --- probability of the \"ngram\"\n",
        "    '''\n",
        "    s_prob = 0\n",
        "    V = len(unigram_dic) + 1 \n",
        "    ### YOUR CODE HERE\\、\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "    # print(ngram)\n",
        "    # try:\n",
        "    #   if len(ngram) == 1:\n",
        "    #     if (ngram[0],) not in unigram_dict:\n",
        "    #       numerator = 0\n",
        "    #       denominator = num_words\n",
        "    #     else:\n",
        "    #       numerator = unigram_dict[(ngram[0],)]\n",
        "    #       denominator = num_words\n",
        "    #   elif len(ngram) == 2:\n",
        "    #     if (ngram[0],) not in unigram_dict and (ngram[0],ngram[1]) not in bigram_dict:\n",
        "    #       numerator = 0\n",
        "    #       denominator = 0\n",
        "    #     elif (ngram[0],ngram[1]) not in bigram_dict:\n",
        "    #       numerator = 0\n",
        "    #     elif (ngram[0],) not in unigram_dict:\n",
        "    #       denominator = 0\n",
        "    #     else:\n",
        "    #       numerator = bigram_dict[(ngram[0],ngram[1])]\n",
        "    #       denominator = unigram_dic[(ngram[0],)]\n",
        "    #   elif len(ngram) == 3:\n",
        "    #     if (ngram[0],ngram[1]) not in bigram_dict and (ngram[0],ngram[1],ngram[2]) not in trigram_dict:\n",
        "    #       numerator = 0\n",
        "    #       denominator = 0\n",
        "    #     elif (ngram[0],ngram[1]) not in bigram_dict:\n",
        "    #       denominator = 0\n",
        "    #     elif (ngram[0],ngram[1],ngram[2]) not in trigram_dict:\n",
        "    #       numerator = 0\n",
        "    #     else:\n",
        "    #       numerator = trigram_dict[(ngram[0],ngram[1],ngram[2])]\n",
        "    #       denominator = bigram_dict[(ngram[0],ngram[1])]\n",
        "    #   numerator += k\n",
        "    #   denominator += V*k\n",
        "    #   s_prob = numerator/denominator\n",
        "    # except Exception as e:\n",
        "    #   print(e)\n",
        "\n",
        "    try:\n",
        "      if len(ngram) == 1:\n",
        "        numerator = unigram_dict.get((ngram[0],),0)\n",
        "        denominator = num_words\n",
        "      elif len(ngram) == 2:\n",
        "        numerator = bigram_dict.get((ngram[0],ngram[1]),0)\n",
        "        denominator = unigram_dict.get((ngram[0],),0)\n",
        "      elif len(ngram) == 3:\n",
        "        numerator = trigram_dict.get((ngram[0],ngram[1],ngram[2]),0)\n",
        "        denominator = bigram_dict.get((ngram[0],ngram[1]),0)\n",
        "      numerator += k\n",
        "      denominator += V*k\n",
        "      s_prob = numerator/denominator\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "\n",
        "    ### END OF YOUR CODE\n",
        "    return s_prob"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvlaDQ66F-2D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d2b8516-902d-492b-b05b-8fee7b0c6854"
      },
      "source": [
        "### ~ 9.31e-05\n",
        "smooth_ngram_prob(ngrams[0], 0.5, num_words, unigram_dict, bigram_dict, trigram_dict)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.311982452086402e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoXGscz-1aHU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "691ac808-ca0b-4fa4-8a86-f4597a391c0c"
      },
      "source": [
        "for ng in ngrams:\n",
        "  print(smooth_ngram_prob(ng,0.5, num_words,unigram_dict, bigram_dict, trigram_dict))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.311982452086402e-05\n",
            "0.00274418131923976\n",
            "0.0024826354988981563\n",
            "0.06726401689559053\n",
            "3.169672572823227e-05\n",
            "0.02127371731998512\n",
            "0.0005184033177812338\n",
            "0.00437373006853325\n",
            "3.4584125886218224e-05\n",
            "3.456738912509938e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pQFfZStF-2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def perplexity(n, k, num_words, valid_sents, unigram_dic, bigram_dic, trigram_dic):\n",
        "    '''\n",
        "    compute the perplexity of valid_sents\n",
        "    params:\n",
        "        n: int --- n-gram model you choose. \n",
        "        k: float --- smoothing parameter.\n",
        "        num_words: int --- total number of words in the traning set.\n",
        "        valid_sents: list[list[str]] --- list of sentences.\n",
        "        unigram_dic: dict{ngram: counts} --- a dictionary that maps each 1-gram to its number of occurences in \"sents\";\n",
        "        bigram_dic: dict{ngram: counts} --- a dictionary that maps each 2-gram to its number of occurence in \"sents\";\n",
        "        trigram_dic: dict{ngram: counts} --- a dictionary that maps each 3-gram to its number occurence in \"sents\";\n",
        "    return:\n",
        "        ppl: float --- perplexity of valid_sents\n",
        "    '''\n",
        "    ppl = None\n",
        "    ### YOUR CODE HERE\n",
        "    # print(len(valid_sents))\n",
        "    total = 0\n",
        "    test_words = 0\n",
        "    for sentence in range(len(valid_sents)):\n",
        "      test_words += len(valid_sents[sentence])\n",
        "      ngrams = list(zip(*[valid_sents[sentence][i:] for i in range(n)]))\n",
        "      # print(ngrams)\n",
        "      for ng in ngrams:\n",
        "        # print(ng)\n",
        "        # print(smooth_ngram_prob(ng, k, num_words, unigram_dic, bigram_dic, trigram_dic))\n",
        "        total += math.log(smooth_ngram_prob(ng, k, num_words, unigram_dic, bigram_dic, trigram_dic),2.0)\n",
        "    l = 1/test_words\n",
        "    ppl = 2**(l*total*-1)\n",
        "    # print(valid_sents[0])\n",
        "    ### END OF YOUR CODE\n",
        "    return ppl"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1i8IqdsF-2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48677b82-4f17-4d3d-a7e7-bbe0f3801a48"
      },
      "source": [
        "### ~ 840\n",
        "perplexity(1, 0.1, num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "840.7347306217125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpcrSZ1dF-2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "a4878c97-5bd3-4260-edf1-17c5b1c4cd06"
      },
      "source": [
        "n = [1,2,3]\n",
        "k = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "### YOUR CODE HEREb\n",
        "best = 10000000000000\n",
        "for i in range(len(n)):\n",
        "  for j in range(len(k)):\n",
        "    print('------------------- N = {} K = {} --------------------'.format(n[i],k[j]))\n",
        "    if n[i]==1:\n",
        "      res = perplexity(n[i], k[j], num_words, uni_valid_sents, unigram_dict, bigram_dict, trigram_dict)\n",
        "      best = min(best,res)\n",
        "      print(res)\n",
        "    elif n[i]==2:\n",
        "      res = perplexity(n[i], k[j], num_words, bi_valid_sents, unigram_dict, bigram_dict, trigram_dict)\n",
        "      best = min(best,res)\n",
        "      print(res)\n",
        "    elif n[i]==3:\n",
        "      res = perplexity(n[i], k[j], num_words, tri_valid_sents, unigram_dict, bigram_dict, trigram_dict)\n",
        "      best = min(best,res)\n",
        "      print(res)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "### END OF YOUR CODE"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------- N = 1 K = 0.1 --------------------\n",
            "840.7347306217125\n",
            "------------------- N = 1 K = 0.3 --------------------\n",
            "841.1427277044075\n",
            "------------------- N = 1 K = 0.5 --------------------\n",
            "841.5959678936316\n",
            "------------------- N = 1 K = 0.7 --------------------\n",
            "842.0904494786319\n",
            "------------------- N = 1 K = 0.9 --------------------\n",
            "842.6227084935349\n",
            "------------------- N = 2 K = 0.1 --------------------\n",
            "739.5817358293406\n",
            "------------------- N = 2 K = 0.3 --------------------\n",
            "1061.3982617375789\n",
            "------------------- N = 2 K = 0.5 --------------------\n",
            "1289.1491260338778\n",
            "------------------- N = 2 K = 0.7 --------------------\n",
            "1477.190939955735\n",
            "------------------- N = 2 K = 0.9 --------------------\n",
            "1641.5907324574955\n",
            "------------------- N = 3 K = 0.1 --------------------\n",
            "4773.649128295989\n",
            "------------------- N = 3 K = 0.3 --------------------\n",
            "6676.617325676175\n",
            "------------------- N = 3 K = 0.5 --------------------\n",
            "7831.228457980847\n",
            "------------------- N = 3 K = 0.7 --------------------\n",
            "8684.056079338212\n",
            "------------------- N = 3 K = 0.9 --------------------\n",
            "9364.604903261927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri8JvJ5QNUCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHRVVmihoFrb",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "\n",
        "Therefore, the best combination would be **N = 2, K = 0.1** with 739.58 perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3o_yjh9lCdXg"
      },
      "source": [
        "### Question 4 [code]\n",
        "\n",
        "Evaluate the perplexity of the test data **test_sents** based on the best n-gram model and $k$ you have found on the validation data (Q 3.3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i7hGcXgCCdXg",
        "colab": {}
      },
      "source": [
        "with open('data/wikitext-2-v1/wikitext-2/wiki.test.tokens', 'r', encoding='utf8') as f:\n",
        "    text = f.readlines()\n",
        "    test_sents = [line.lower().strip('\\n').split() for line in text]\n",
        "    test_sents = [s for s in test_sents if len(s)>0 and s[0] != '=']\n",
        "\n",
        "uni_test_sents = pad_sents(test_sents, 1)\n",
        "bi_test_sents = pad_sents(test_sents, 2)\n",
        "tri_test_sents = pad_sents(test_sents, 3)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXV0ILeqF-2O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3461605-7af6-4838-ca8c-0d4a41b8b2c2"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "perplexity(2, 0.1, num_words,bi_test_sents, unigram_dict, bigram_dict, trigram_dict)\n",
        "### END OF YOUR CODE"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "689.3929590954306"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "ePSI8RDWCdXj"
      },
      "source": [
        "## Neural Language Model (RNN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jkoTco_jCdXj"
      },
      "source": [
        "<img src=\"LM.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
        "\n",
        "We will create a LSTM language model as shown in figure and train it on the Wikitext-2 dataset. \n",
        "The data generators (train\\_iter, valid\\_iter, test\\_iter) have been provided. \n",
        "The word embeddings together with the parameters in the LSTM model will be learned from scratch.\n",
        "\n",
        "[Pytorch](https://pytorch.org/tutorials/) and [torchtext](https://torchtext.readthedocs.io/en/latest/index.html#) are required in this part. Do not make any changes to the provided code unless you are requested to do so. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9eVySQWF-2Q",
        "colab_type": "text"
      },
      "source": [
        "### Question 5 [code]\n",
        "- Implement the ``__init__`` function in ``LangModel`` class.\n",
        "- Implement the ``forward`` function in ``LangModel`` class.\n",
        "- Complete the training code in ``train`` function.\n",
        "    Then complete the testing code in  ``test`` function and \n",
        "    compute the perplexity of the test data ``test_iter``. The test perplexity should be below 150."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tF2GCRdFCdXk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b329b053-0a33-4b07-e8a2-bfc1deaf4448"
      },
      "source": [
        "import torchtext\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.datasets import WikiText2\n",
        "from torch import nn, optim\n",
        "from torchtext import data\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "torch.manual_seed(222)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe96c053c30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lt6JBdSnCdXn",
        "colab": {}
      },
      "source": [
        "def tokenizer(text):\n",
        "    '''Tokenize a string to words'''\n",
        "    return word_tokenize(text)\n",
        "\n",
        "START = '<START>'\n",
        "STOP = '<STOP>'\n",
        "#Load and split data into three parts\n",
        "TEXT = data.Field(lower=True, tokenize=tokenizer, init_token=START, eos_token=STOP)\n",
        "train, valid, test = WikiText2.splits(TEXT) "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KHwu4VMVCdXq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a7a1723-6777-4057-f65e-0357a14d7619"
      },
      "source": [
        "#Build a vocabulary from the train dataset\n",
        "TEXT.build_vocab(train)\n",
        "print('Vocabulary size:', len(TEXT.vocab))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 28908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ua9e-OBMCdXs",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "# the length of a piece of text feeding to the RNN layer\n",
        "BPTT_LEN = 32           \n",
        "# train, validation, test data\n",
        "train_iter, valid_iter, test_iter = data.BPTTIterator.splits((train, valid, test),\n",
        "                                                                batch_size=BATCH_SIZE,\n",
        "                                                                bptt_len=BPTT_LEN,\n",
        "                                                                repeat=False)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6_Cd8shXCdXy",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6b7b411d-b7d4-44a4-9f95-6bf6d72d2cfe"
      },
      "source": [
        "#Generate a batch of train data\n",
        "batch = next(iter(train_iter))\n",
        "text, target = batch.text, batch.target\n",
        "# print(batch.dataset[0].text[:32])\n",
        "# print(text[0:3],target[:3])\n",
        "print('Size of text tensor',text.size())\n",
        "print('Size of target tensor',target.size())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of text tensor torch.Size([32, 64])\n",
            "Size of target tensor torch.Size([32, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC6WX7feF-2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LangModel(nn.Module):\n",
        "    def __init__(self, lang_config):\n",
        "        super(LangModel, self).__init__()\n",
        "        self.vocab_size = lang_config['vocab_size']\n",
        "        self.emb_size = lang_config['emb_size']\n",
        "        self.hidden_size = lang_config['hidden_size']\n",
        "        self.num_layer = lang_config['num_layer']\n",
        "        \n",
        "        self.embedding = None\n",
        "        self.rnn = None\n",
        "        self.linear = None\n",
        "        \n",
        "        ### TODO: \n",
        "        ###    1. Initialize 'self.embedding' with nn.Embedding function and 2 variables we have initialized for you\n",
        "        ###    2. Initialize 'self.rnn' with nn.LSTM function and 3 variables we have initialized for you\n",
        "        ###    3. Initialize 'self.linear' with nn.Linear function and 2 variables we have initialized for you\n",
        "        ### Reference:\n",
        "        ###        https://pytorch.org/docs/stable/nn.html\n",
        "        \n",
        "        ### YOUR CODE HERE (3 lines)\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size,embedding_dim=self.emb_size)\n",
        "        self.rnn = nn.LSTM(input_size=self.emb_size,hidden_size=self.hidden_size,num_layers=self.num_layer)\n",
        "        self.linear = nn.Linear(in_features=self.hidden_size,out_features=self.vocab_size)\n",
        "\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "        \n",
        "    def forward(self, batch_sents, hidden=None):\n",
        "        '''\n",
        "        params:\n",
        "            batch_sents: torch.LongTensor of shape (sequence_len, batch_size)\n",
        "        return:\n",
        "            normalized_score: torch.FloatTensor of shape (sequence_len, batch_size, vocab_size)\n",
        "        '''\n",
        "        normalized_score = None\n",
        "        hidden = hidden\n",
        "        ### TODO:\n",
        "        ###      1. Feed the batch_sents to self.embedding  \n",
        "        ###      2. Feed the embeddings to self.rnn. Remember to pass \"hidden\" into self.rnn, even if it is None. But we will \n",
        "        ###         use \"hidden\" when implementing greedy search.\n",
        "        ###      3. Apply linear transformation to the output of self.rnn\n",
        "        ###      4. Apply 'F.log_softmax' to the output of linear transformation\n",
        "        ###\n",
        "        ### YOUR CODE HERE \n",
        "        batch_sents = self.embedding(batch_sents)\n",
        "        batch_sents,hidden = self.rnn(batch_sents,hidden)\n",
        "        batch_sents = self.linear(batch_sents)\n",
        "        normalized_score = F.log_softmax(batch_sents,dim=2)\n",
        "\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "        return normalized_score, hidden"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhbdiqRAF-2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_iter, valid_iter, vocab_size, criterion, optimizer, num_epochs):\n",
        "    for n in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        target_num = 0\n",
        "        model.train()\n",
        "        for batch in train_iter:\n",
        "            \n",
        "            text, targets = batch.text.to(device), batch.target.to(device)\n",
        "            loss = None\n",
        "            \n",
        "            ### we don't consider \"hidden\" here. So according to the default setting, \"hidden\" will be None\n",
        "            ### YOU CODE HERE (~5 lines)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            prediction,_ = model(text)\n",
        "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            ### END OF YOUR CODE\n",
        "            ##########################################\n",
        "            train_loss += loss.item() * targets.size(0) * targets.size(1)\n",
        "            target_num += targets.size(0) * targets.size(1)\n",
        "\n",
        "        train_loss /= target_num\n",
        "\n",
        "        # monitor the loss of all the predictions\n",
        "        val_loss = 0\n",
        "        target_num = 0\n",
        "        model.eval()\n",
        "        for batch in valid_iter:\n",
        "            text, targets = batch.text.to(device), batch.target.to(device)\n",
        "            \n",
        "            prediction,_ = model(text)\n",
        "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
        "            \n",
        "            val_loss += loss.item() * targets.size(0) * targets.size(1)\n",
        "            target_num += targets.size(0) * targets.size(1)\n",
        "        val_loss /= target_num\n",
        "\n",
        "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(n+1, train_loss, val_loss))            "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX-ZQwWZF-2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def test(model, vocab_size, criterion, test_iter):\n",
        "    '''\n",
        "    params: \n",
        "        model: LSTM model\n",
        "        test_iter: test data\n",
        "    return:\n",
        "        ppl: perplexity \n",
        "    '''\n",
        "    ppl = None\n",
        "    test_loss = 0\n",
        "    target_num = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            text, targets = batch.text.to(device), batch.target.to(device)\n",
        "\n",
        "            prediction,_ = model(text)\n",
        "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "            test_loss += loss.item() * targets.size(0) * targets.size(1)\n",
        "            target_num += targets.size(0) * targets.size(1)\n",
        "\n",
        "        test_loss /= target_num\n",
        "        \n",
        "        ### Compute perplexity according to \"test_loss\"\n",
        "        ### Hint: Consider how the loss is computed.\n",
        "        ### YOUR CODE HERE(1 line)\n",
        "        ppl = math.exp(test_loss)\n",
        "        ### END OF YOUR CODE\n",
        "        return ppl"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSjhIWipF-2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs=10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = len(TEXT.vocab)\n",
        "\n",
        "config = {'vocab_size':vocab_size,\n",
        "         'emb_size':128,\n",
        "         'hidden_size':128,\n",
        "         'num_layer':1}\n",
        "\n",
        "LM = LangModel(config)\n",
        "LM = LM.to(device)\n",
        "\n",
        "criterion = nn.NLLLoss(reduction='mean')\n",
        "optimizer = optim.Adam(LM.parameters(), lr=1e-3, betas=(0.7, 0.99))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2yjsdH9F-2g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "321c7657-c5ac-48d3-b231-3f6081006705"
      },
      "source": [
        "train(LM, train_iter, valid_iter,vocab_size, criterion, optimizer, num_epochs)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training Loss: 6.0691, Validation Loss: 5.1777\n",
            "Epoch: 2, Training Loss: 5.4015, Validation Loss: 4.9643\n",
            "Epoch: 3, Training Loss: 5.1293, Validation Loss: 4.8661\n",
            "Epoch: 4, Training Loss: 4.9561, Validation Loss: 4.8139\n",
            "Epoch: 5, Training Loss: 4.8310, Validation Loss: 4.7835\n",
            "Epoch: 6, Training Loss: 4.7311, Validation Loss: 4.7638\n",
            "Epoch: 7, Training Loss: 4.6476, Validation Loss: 4.7500\n",
            "Epoch: 8, Training Loss: 4.5765, Validation Loss: 4.7421\n",
            "Epoch: 9, Training Loss: 4.5150, Validation Loss: 4.7385\n",
            "Epoch: 10, Training Loss: 4.4606, Validation Loss: 4.7391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HldvN8ziF-2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b7f2949-a3e7-4ad6-ddbb-c99ad33caa21"
      },
      "source": [
        "# < 150\n",
        "test(LM, vocab_size, criterion, test_iter)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.14565658706458"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeV75KSFF-2l",
        "colab_type": "text"
      },
      "source": [
        "### Question 6 [code]\n",
        "When we use trained language model to generate a sentence given a start token, we can choose either ``greedy search`` or ``beam search``. \n",
        "<img src=\"greedy.png\" alt=\"drawing\" style=\"width:500px;\"/>\n",
        "\n",
        "As shown above, ``greedy search`` algorithm will pick the token which has the highest probability and feed it to the language model as input in the next time step. The model will generate ``max_len`` number of tokens at most.\n",
        "\n",
        "- Implement ``word_greedy_search``\n",
        "- **[optional]** Implement ``word_beam_search`` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTQZjcQ7F-2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_greedy_search(model, start_token, max_len):\n",
        "    '''\n",
        "    param:\n",
        "        model: nn.Module --- language model\n",
        "        start_token: str --- e.g. 'he'\n",
        "        max_len: int --- max number of tokens generated\n",
        "    return:\n",
        "        strings: list[str] --- list of tokens, e.g., ['he', 'was', 'a', 'member', 'of',...]\n",
        "    '''\n",
        "    model.eval()\n",
        "    ID = TEXT.vocab.stoi[start_token]\n",
        "    strings = [start_token]\n",
        "    hidden = None\n",
        "    \n",
        "    ### You may find TEXT.vocab.itos useful.\n",
        "    ### YOUR CODE HERE\n",
        "\n",
        "    end = TEXT.vocab.stoi[\"<eos>\"]\n",
        "    # print(end)\n",
        "    hidden = None\n",
        "    for _ in range(max_len):\n",
        "      norm,hidden = model(torch.tensor([ID]).unsqueeze(1).to(device),hidden)\n",
        "      ID =  torch.argmax(norm)\n",
        "      nextword = TEXT.vocab.itos[ID.item()]\n",
        "      strings.append(nextword)\n",
        "      # print(strings)\n",
        "      if(ID.item()== end):\n",
        "        break\n",
        "\n",
        "    ### END OF YOUR CODE \n",
        "    return strings"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPWF1o5qF-2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BeamNode = namedtuple('BeamNode', ['prev_node', 'prev_hidden', 'wordID', 'score', 'length'])\n",
        "# LMNode = namedtuple('LMNode', ['sent', 'score'])\n",
        "\n",
        "def word_beam_search(model, start_token, max_len, beam_size):\n",
        "    model.eval()\n",
        "    ID = TEXT.vocab.stoi[start_token]\n",
        "    strings = [start_token]\n",
        "    hidden = None\n",
        "\n",
        "\n",
        "    k_beam = [(0, [0]*(max_len+1))]\n",
        "\n",
        "    # l : point on target sentence to predict\n",
        "    for l in range(max_len):\n",
        "        all_k_beams = []\n",
        "        for prob, sent_predict in k_beam:\n",
        "            # predicted = model.predict([np.array([src_input]), np.array([sent_predict])])[0]\n",
        "            # # top k!\n",
        "            # possible_k = predicted[l].argsort()[-beam_size:][::-1]\n",
        "            norm,hidden = model(torch.tensor([ID]).unsqueeze(1).to(device),hidden)\n",
        "            possible_k = torch.topk(norm,beam_size)\n",
        "            # print(possible_k)\n",
        "            ID = possible_k.indices[0][0].tolist()\n",
        "            print(ID)\n",
        "\n",
        "            # add to all possible candidates for k-beams\n",
        "            # all_k_beams += [\n",
        "            #     (\n",
        "            #         sum(np.log(possible_k.values[0][0].tolist()[i][sent_predict[i+1]]) for i in range(l)) + np.log(possible_k.values[0][0].tolist()[i][next_wid]),\n",
        "            #         list(sent_predict[:l+1])+[next_wid]+[0]*(max_len-l-1)\n",
        "            #     )\n",
        "            #     for next_wid in possible_k.indices[0][0].tolist()\n",
        "            # ]\n",
        "            for next_wid in possible_k.indices[0][0].tolist():\n",
        "              for i in range(l):\n",
        "                all_k_beams += [\n",
        "                  (\n",
        "                      sum(np.log(possible_k.values[0][0].tolist()[i][sent_predict[i+1]]) ) + np.log(possible_k.values[0][0].tolist()[i][next_wid]),\n",
        "                      list(sent_predict[:l+1])+[next_wid]+[0]*(max_len-l-1)\n",
        "                  )]\n",
        "                print(all_k_beams)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "        # top k\n",
        "            k_beam = sorted(all_k_beams)[-beam_size:]\n",
        "\n",
        "    return k_beam\n",
        "    \n",
        "    ### You may find TEXT.vocab.itos useful.\n",
        "    ### YOUR CODE HERE\n",
        "    # ret = []\n",
        "    # end = TEXT.vocab.stoi[\"<eos>\"]\n",
        "    # sequences = [[list(), 0.0]]\n",
        "    # # print(end)\n",
        "    # hidden = None\n",
        "    # norm,hidden = model(torch.tensor([ID]).unsqueeze(1).to(device),hidden)\n",
        "    # ID = torch.topk(norm,beam_size)\n",
        "    # for _ in range(1,max_len):\n",
        "    #   all_candidates = list()\n",
        "    #   for i in range(1,max_len):\n",
        "    #     for j in range(beam_size):\n",
        "    #       norm,hidden = model(torch.tensor([ID.indices[0][0][j]]).unsqueeze(1).to(device),hidden)\n",
        "    #       ID = torch.topk(norm,beam_size)\n",
        "    #       candidate = [seq + ID.indices[0][0][j], ID.scores[0][0][j] - log(row[j])]\n",
        "    #       all_candidates.append(candidate)\n",
        "      # ordered = torch.sort(norm,descending= True)\n",
        "      # print(ordered[:beam_size])\n",
        "          \n",
        "\n",
        "      # ID =  torch.argmax(norm)\n",
        "      # nextword = TEXT.vocab.itos[ID.item()]\n",
        "      # strings.append(nextword)\n",
        "      # all_candidates = list()\n",
        "      # for i in range(len(sequences)):\n",
        "      #   seq, score = sequences[i]\n",
        "      #   for j in range(len(row)):\n",
        "      #     candidate = [seq + [j], score - log(row[j])]\n",
        "      #     all_candidates.append(candidate)\n",
        "      # if(ID.item()== end):\n",
        "      #   break\n",
        "      # ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "\t\t# select k best\n",
        "      # sequences = ordered[:beam_size]\n",
        "\n",
        "    return "
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S8JTELO6ZDQ",
        "colab_type": "text"
      },
      "source": [
        "## Result\n",
        "\n",
        "I tried to implement the beam search, but was not able to produce the desired output. I tried by finding the topk and adding the log likelihood of all the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs6kote0F-2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47bdf23e-9ac6-41a6-c318-cf2ae482986e"
      },
      "source": [
        "word_greedy_search(LM, 'he', 64)  "
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he', 'was', 'a', 'member', 'of', 'the', '<', 'unk', '>', '.', '<eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_w0UKI0F-2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e4fda0c9-77d3-4877-a7d9-ebec5f9ed36a"
      },
      "source": [
        "word_beam_search(LM, 'he', 64, 3)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[19, 38, 30]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0b7Bbs0F-2t",
        "colab_type": "text"
      },
      "source": [
        "# char-level LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4jIpoGZF-2u",
        "colab_type": "text"
      },
      "source": [
        "### Question 7 [code]\n",
        "- Implement ``char_tokenizer``\n",
        "- Implement ``CharLangModel``, ``char_train``, ``char_test``\n",
        "- Implement ``char_greedy_search``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYr5EDBKF-2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def char_tokenizer(string):\n",
        "    '''\n",
        "    param:\n",
        "        string: str --- e.g. \"I love this assignment\"\n",
        "    return:\n",
        "        char_list: list[str] --- e.g. ['I', 'l', 'o', 'v', 'e', ' ', 't', 'h', 'i', 's', ...]\n",
        "    '''\n",
        "    char_list = None\n",
        "    ### YOUR CODE HERE\n",
        "    char_list = []\n",
        "    [char_list.append(x) for x in string]\n",
        "    ### END OF YOUR CODE\n",
        "    return char_list"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nilFgH8wF-2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0cc5dc99-5e86-44f1-dc2e-c42f33caa68a"
      },
      "source": [
        "test_str = 'test test test'\n",
        "char_tokenizer(test_str)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t', 'e', 's', 't', ' ', 't', 'e', 's', 't', ' ', 't', 'e', 's', 't']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkRvyX9qF-2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHAR_TEXT = data.Field(lower=True, tokenize=char_tokenizer ,init_token='<START>', eos_token='<STOP>')\n",
        "ctrain, cvalid, ctest = WikiText2.splits(CHAR_TEXT)  "
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBr5B9K9F-20",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ea8d5641-a5b4-4205-ff2c-9acc11583b5a"
      },
      "source": [
        "CHAR_TEXT.build_vocab(ctrain)\n",
        "print('Vocabulary size:', len(CHAR_TEXT.vocab))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AobIRP-1F-21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "# the length of a piece of text feeding to the RNN layer\n",
        "BPTT_LEN = 128        \n",
        "# train, validation, test data\n",
        "ctrain_iter, cvalid_iter, ctest_iter = data.BPTTIterator.splits((ctrain, cvalid, ctest),\n",
        "                                                                batch_size=BATCH_SIZE,\n",
        "                                                                bptt_len=BPTT_LEN,\n",
        "                                                                repeat=False)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga6VhIRxF-23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharLangModel(nn.Module):\n",
        "    def __init__(self, lang_config):\n",
        "        ### YOUR CODE HERE\n",
        "        super(CharLangModel, self).__init__()\n",
        "        self.vocab_size = lang_config['vocab_size']\n",
        "        self.emb_size = lang_config['emb_size']\n",
        "        self.hidden_size = lang_config['hidden_size']\n",
        "        self.num_layer = lang_config['num_layer']\n",
        "        \n",
        "        self.embedding = None\n",
        "        self.rnn = None\n",
        "        self.linear = None\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size,embedding_dim=self.emb_size)\n",
        "        self.rnn = nn.LSTM(input_size=self.emb_size,hidden_size=self.hidden_size,num_layers=self.num_layer)\n",
        "        self.linear = nn.Linear(in_features=self.hidden_size,out_features=self.vocab_size)\n",
        "        \n",
        "    def forward(self, batch_sents, hidden):\n",
        "        ### YOUR CODE HERE\n",
        "        batch_sents = self.embedding(batch_sents)\n",
        "        batch_sents,hidden = self.rnn(batch_sents,hidden)\n",
        "        batch_sents = self.linear(batch_sents)\n",
        "        normalized_score = F.log_softmax(batch_sents,dim=2)\n",
        "\n",
        "\n",
        "        ### END OF YOUR CODE\n",
        "        return normalized_score, hidden\n",
        "        "
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJOxDHf1F-24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def char_train(model, train_iter, valid_iter, criterion, optimizer, vocab_size, num_epochs):\n",
        "    for n in range(num_epochs):\n",
        "        train_loss = 0\n",
        "        target_num = 0\n",
        "        model.train()\n",
        "        for batch in train_iter:\n",
        "            \n",
        "            text, targets = batch.text.to(device), batch.target.to(device)\n",
        "            loss = None\n",
        "            \n",
        "            ### we don't consider \"hidden\" here. So according to the default setting, \"hidden\" will be None\n",
        "            ### YOU CODE HERE (~5 lines)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            prediction,_ = model(text,None)\n",
        "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            ### END OF YOUR CODE\n",
        "            ##########################################\n",
        "            train_loss += loss.item() * targets.size(0) * targets.size(1)\n",
        "            target_num += targets.size(0) * targets.size(1)\n",
        "\n",
        "        train_loss /= target_num\n",
        "\n",
        "        # monitor the loss of all the predictions\n",
        "        val_loss = 0\n",
        "        target_num = 0\n",
        "        model.eval()\n",
        "        for batch in valid_iter:\n",
        "            text, targets = batch.text.to(device), batch.target.to(device)\n",
        "            \n",
        "            prediction,_ = model(text,None)\n",
        "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
        "            \n",
        "            val_loss += loss.item() * targets.size(0) * targets.size(1)\n",
        "            target_num += targets.size(0) * targets.size(1)\n",
        "        val_loss /= target_num\n",
        "\n",
        "        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(n+1, train_loss, val_loss))   "
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1TgyJb_F-26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def char_test(model, vocab_size, test_iter, criterion):\n",
        "    '''\n",
        "    params: \n",
        "        model: LSTM model\n",
        "        test_iter: test data\n",
        "    return:\n",
        "        ppl: perplexity \n",
        "    '''\n",
        "    ppl = None\n",
        "    test_loss = 0\n",
        "    target_num = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            text, targets = batch.text.to(device), batch.target.to(device)\n",
        "\n",
        "            prediction,_ = model(text,None)\n",
        "            loss = criterion(prediction.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "            test_loss += loss.item() * targets.size(0) * targets.size(1)\n",
        "            target_num += targets.size(0) * targets.size(1)\n",
        "\n",
        "        test_loss /= target_num\n",
        "        \n",
        "        ### Compute perplexity according to \"test_loss\"\n",
        "        ### Hint: Consider how the loss is computed.\n",
        "        ### YOUR CODE HERE(1 line)\n",
        "        ppl = math.exp(test_loss)\n",
        "        ### END OF YOUR CODE\n",
        "        return ppl"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmoNzFTqF-28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs=10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "char_vocab_size = len(CHAR_TEXT.vocab)\n",
        "\n",
        "config = {'vocab_size':char_vocab_size,\n",
        "         'emb_size':128,\n",
        "         'hidden_size':128,\n",
        "         'num_layer':1}\n",
        "\n",
        "CLM = CharLangModel(config)\n",
        "CLM = CLM.to(device)\n",
        "\n",
        "char_criterion = nn.NLLLoss(reduction='mean')\n",
        "char_optimizer = optim.Adam(CLM.parameters(), lr=1e-3, betas=(0.7, 0.99))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfZmuOboF-2-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "a0513d97-6b3d-4198-d5a5-ede9de8ce370"
      },
      "source": [
        "char_train(CLM, ctrain_iter, cvalid_iter, char_criterion, char_optimizer, char_vocab_size, num_epochs)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training Loss: 1.8418, Validation Loss: 1.5487\n",
            "Epoch: 2, Training Loss: 1.5469, Validation Loss: 1.4422\n",
            "Epoch: 3, Training Loss: 1.4730, Validation Loss: 1.3966\n",
            "Epoch: 4, Training Loss: 1.4354, Validation Loss: 1.3714\n",
            "Epoch: 5, Training Loss: 1.4115, Validation Loss: 1.3548\n",
            "Epoch: 6, Training Loss: 1.3946, Validation Loss: 1.3425\n",
            "Epoch: 7, Training Loss: 1.3818, Validation Loss: 1.3323\n",
            "Epoch: 8, Training Loss: 1.3713, Validation Loss: 1.3239\n",
            "Epoch: 9, Training Loss: 1.3628, Validation Loss: 1.3170\n",
            "Epoch: 10, Training Loss: 1.3559, Validation Loss: 1.3114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkECM2HiF-3B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "affdaa55-fb18-4bcc-f44b-7830aaa66250"
      },
      "source": [
        "# <10\n",
        "char_test(CLM, char_vocab_size, ctest_iter, char_criterion)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.685492545227015"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6meA1FtF-3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def char_greedy_search(model, start_token, max_len):\n",
        "    '''\n",
        "    param:\n",
        "        model: nn.Module --- language model\n",
        "        start_token: str --- e.g. 'h'\n",
        "        max_len: int --- max number of tokens generated\n",
        "    return:\n",
        "        strings: list[str] --- list of tokens, e.g., ['h', 'e', ' ', 'i', 's',...]\n",
        "    '''   \n",
        "    model.eval()\n",
        "    ID = CHAR_TEXT.vocab.stoi[start_token]\n",
        "    strings = [start_token]\n",
        "    hidden = None\n",
        "    end = CHAR_TEXT.vocab.stoi[\"<eos>\"]\n",
        "    # print(end)\n",
        "    hidden = None\n",
        "    for _ in range(1,max_len):\n",
        "      norm,hidden = model(torch.tensor([ID]).unsqueeze(1).to(device),hidden)\n",
        "      ID =  torch.argmax(norm)\n",
        "      nextword = CHAR_TEXT.vocab.itos[ID.item()]\n",
        "      strings.append(nextword)\n",
        "      # print(strings)\n",
        "      if(ID.item()== end):\n",
        "        break\n",
        "    ### END OF YOUR CODE \n",
        "    return strings"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qELvI_fj7qcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3512262-a456-4bb7-98b8-7795f003e2df"
      },
      "source": [
        "char_greedy_search(CLM, 'h', 64)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['h',\n",
              " 'e',\n",
              " ' ',\n",
              " 's',\n",
              " 't',\n",
              " 'a',\n",
              " 't',\n",
              " 'e',\n",
              " ' ',\n",
              " ',',\n",
              " ' ',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 's',\n",
              " 'e',\n",
              " 'c',\n",
              " 'o',\n",
              " 'n',\n",
              " 'd',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 's',\n",
              " 't',\n",
              " 'o',\n",
              " 'r',\n",
              " 'y',\n",
              " ' ',\n",
              " ',',\n",
              " ' ',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 's',\n",
              " 't',\n",
              " 'o',\n",
              " 'r',\n",
              " 'y',\n",
              " ' ',\n",
              " ',',\n",
              " ' ',\n",
              " 'a',\n",
              " 'n',\n",
              " 'd',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 's',\n",
              " 't']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "collapsed": true,
        "id": "cernoBq6CdX9"
      },
      "source": [
        "### Requirements:\n",
        "- This is an individual report.\n",
        "- Complete the code using Python.\n",
        "- List students with whom you have discussed if there are any.\n",
        "- Follow the honor code strictly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bDOUuC7-CdX-"
      },
      "source": [
        "### Free GPU Resources\n",
        "We suggest that you run neural language models on machines with GPU(s). Google provides the free online platform [Colaboratory](https://colab.research.google.com/notebooks/welcome.ipynb), a research tool for machine learning education and research. It’s a Jupyter notebook environment that requires no setup to use as common packages have been  pre-installed. Google users can have access to a Tesla T4 GPU (approximately 15G memory). Note that when you connect to a GPU-based VM runtime, you are given a maximum of 12 hours at a time on the VM.\n",
        "\n",
        "It is convenient to upload local Jupyter Notebook files and data to Colab, please refer to the [tutorial](https://colab.research.google.com/notebooks/io.ipynb). \n",
        "\n",
        "In addition, Microsoft also provides the online platform [Azure Notebooks](https://notebooks.azure.com/help/introduction) for research of data science and machine learning, there are free trials for new users with credits."
      ]
    }
  ]
}